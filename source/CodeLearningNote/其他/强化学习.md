---
hidden: true
---


[莫烦Python](mofanpy.com)

## QLearning

![整体算法](https://static.mofanpy.com/static/results/ML-intro/q4.png?t=6735b03a&sign=d2e60c542fb57754a759b01331e1fda9)

![qlrearning公式](https://wikimedia.org/api/rest_v1/media/math/render/svg/a3a4d2ac903b1be02cc81e60de2e9f91d7025fec)

Q(state, action) : 在当前state做出该action的价值(此价值包含从当前点直到最终点)

![aaaa](https://static.mofanpy.com/static/results/ML-intro/q5.png?t=6735b03a&sign=c9592952191e05b46abc14be9e5d07ff)

### DQLearning

state太多(如下围棋)

在机器学习中, 有一种方法对这种事情很在行, 那就是神经网络. 我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 

## Sarsa

![aaa](https://static.mofanpy.com/static/results/ML-intro/s4.png?t=6735b350&sign=048244833c06e5bedf2c04c95af91ebe)

 on-policy, 下一个 state*, 和下一个 action* 将会变成他真正采取的 action 和 state. 

 不过于 Qlearning 不同之处:

- 他在当前 `state` 已经想好了 `state` 对应的 `action`, 而且想好了 下一个 `state_` 和下一个 `action_` (Qlearning 还没有想好下一个 `action_`)
- 更新 `Q(s,a)` 的时候基于的是下一个 `Q(s_, a_)` (Qlearning 是基于 `maxQ(s_)`)

### sarsa_Lamda

![bbb](https://static.mofanpy.com/static/results/ML-intro/sl5.png?t=6735b350&sign=f9d26d18dda35c5fccfa1c3a047cd594)

 当 lambda 在 0 和 1 之间, 取值越大, 离宝藏越近的步更新力度越大. 这样我们就不用受限于单步更新的每次只能更新最近的一步, 我们可以更有效率的更新所有相关步了.

## Policy Gradient

Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作. 他接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action

用神经网络输出动作

把 reward 看成神经网络中的 loss (要乘个负号) , 进行反向传递, 修改参数

回合更新

## Actor Critic 

Actor Critic (演员评判家)

Actor -> 类policy gradient的输出*行为*的神经网络

Critic -> 类qlearning的输出*值*的神经网络

## PPO

根据 OpenAI 的[官方博客](https://blog.openai.com/openai-baselines-ppo/), PPO 已经成为他们在强化学习上的默认算法. **如果一句话概括 PPO: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题.** 





