<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>lyx12138的博客</title><meta name="robots" content="noindex"><link rel="icon" type="image/x-icon" href="https://cdn.luogu.com.cn/upload/image_hosting/8cc2fgam.png"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"复制"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
 menuSettings: {
   zoom: "None"
 },
 showMathMenu: false,
 jax: ["input/TeX","output/CommonHTML"],
 extensions: ["tex2jax.js"],
 TeX: {
   extensions: ["AMSmath.js","AMSsymbols.js"],
   equationNumbers: {
     autoNumber: "AMS"
   }
 },
 tex2jax: {
   inlineMath: [["\\(", "\\)"]],
   displayMath: [["\\[", "\\]"]]
 }
});</script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
 --light-background: url('/img/bk.jpg');
 --theme-encrypt-confirm: '确认'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
  menuSettings: {
    zoom: "None"
  },
  showMathMenu: false,
  jax: ["input/TeX","output/CommonHTML"],
  extensions: ["tex2jax.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js"],
    equationNumbers: {
      autoNumber: "AMS"
    }
  },
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]]
  }
});
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/Proverb/"><span class="navItemTitle">Proverb</span></a></li><li class="navItem"><a class="navBlock" href="/demo/Arrangement/"><span class="navItemTitle">Arrangement</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1></h1></div><hr><div id="post-content"><p><a href="mofanpy.com">莫烦Python</a></p>
<h2 id="qlearning"><a href="#qlearning" class="headerlink" title="qlearning"></a>QLearning</h2>
<figure>
<img
src="https://static.mofanpy.com/static/results/ML-intro/q4.png?t=6735b03a&amp;sign=d2e60c542fb57754a759b01331e1fda9"
alt="整体算法" />
<figcaption aria-hidden="true">整体算法</figcaption>
</figure>
<figure>
<img
src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3a4d2ac903b1be02cc81e60de2e9f91d7025fec"
alt="qlrearning公式" />
<figcaption aria-hidden="true">qlrearning公式</figcaption>
</figure>
<p>Q(state, action) :
在当前state做出该action的价值(此价值包含从当前点直到最终点)</p>
<figure>
<img
src="https://static.mofanpy.com/static/results/ML-intro/q5.png?t=6735b03a&amp;sign=c9592952191e05b46abc14be9e5d07ff"
alt="aaaa" />
<figcaption aria-hidden="true">aaaa</figcaption>
</figure>
<h3 id="dqlearning"><a href="#dqlearning" class="headerlink" title="dqlearning"></a>DQLearning</h3>
<p>state太多(如下围棋)</p>
<p>在机器学习中, 有一种方法对这种事情很在行, 那就是神经网络.
我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的
Q 值,</p>
<h2 id="sarsa"><a href="#sarsa" class="headerlink" title="sarsa"></a>Sarsa</h2>
<figure>
<img
src="https://static.mofanpy.com/static/results/ML-intro/s4.png?t=6735b350&amp;sign=048244833c06e5bedf2c04c95af91ebe"
alt="aaa" />
<figcaption aria-hidden="true">aaa</figcaption>
</figure>
<p>on-policy, 下一个 state<em>, 和下一个 action</em>
将会变成他真正采取的 action 和 state.</p>
<p>不过于 Qlearning 不同之处:</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的
<code>action</code>, 而且想好了 下一个 <code>state_</code> 和下一个
<code>action_</code> (Qlearning 还没有想好下一个
<code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code>
(Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<h3 id="sarsa_lamda"><a href="#sarsa_lamda" class="headerlink" title="sarsa_lamda"></a>sarsa_Lamda</h3>
<figure>
<img
src="https://static.mofanpy.com/static/results/ML-intro/sl5.png?t=6735b350&amp;sign=f9d26d18dda35c5fccfa1c3a047cd594"
alt="bbb" />
<figcaption aria-hidden="true">bbb</figcaption>
</figure>
<p>当 lambda 在 0 和 1 之间, 取值越大, 离宝藏越近的步更新力度越大.
这样我们就不用受限于单步更新的每次只能更新最近的一步,
我们可以更有效率的更新所有相关步了.</p>
<h2 id="policy-gradient"><a href="#policy-gradient" class="headerlink" title="policy-gradient"></a>Policy Gradient</h2>
<p>Policy Gradients 直接输出动作的最大好处就是,
它能在一个连续区间内挑选动作. 他接受环境信息 (observation),
不同的是他要输出不是 action 的 value, 而是具体的那一个 action</p>
<p>用神经网络输出动作</p>
<p>把 reward 看成神经网络中的 loss (要乘个负号) , 进行反向传递,
修改参数</p>
<p>回合更新</p>
<h2 id="actor-critic"><a href="#actor-critic" class="headerlink" title="actor-critic"></a>Actor Critic</h2>
<p>Actor Critic (演员评判家)</p>
<p>Actor -&gt; 类policy gradient的输出<em>行为</em>的神经网络</p>
<p>Critic -&gt; 类qlearning的输出<em>值</em>的神经网络</p>
<h2 id="ppo"><a href="#ppo" class="headerlink" title="ppo"></a>PPO</h2>
<p>根据 OpenAI 的<a
target="_blank" rel="noopener" href="https://blog.openai.com/openai-baselines-ppo/">官方博客</a>, PPO
已经成为他们在强化学习上的默认算法. <strong>如果一句话概括 PPO: OpenAI
提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size)
的问题.</strong></p>
<!-- flag of hidden posts --><div id="paginator"></div></div><div id="post-footer"></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://cdn.luogu.com.cn/upload/image_hosting/8cc2fgam.png" alt="Logo" style="margin:0;border-radius:0;"></a><h1 id="Dr"><a href="/">lyx12138</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#qlearning"><span class="toc-number">1.</span> <span class="toc-text">QLearning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dqlearning"><span class="toc-number">1.1.</span> <span class="toc-text">DQLearning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sarsa"><span class="toc-number">2.</span> <span class="toc-text">Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sarsa_lamda"><span class="toc-number">2.1.</span> <span class="toc-text">sarsa_Lamda</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#policy-gradient"><span class="toc-number">3.</span> <span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#actor-critic"><span class="toc-number">4.</span> <span class="toc-text">Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ppo"><span class="toc-number">5.</span> <span class="toc-text">PPO</span></a></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>