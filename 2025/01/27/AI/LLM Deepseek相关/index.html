<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>LLM Deepseek相关 | lyx12138的博客</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"复制"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
 menuSettings: {
   zoom: "None"
 },
 showMathMenu: false,
 jax: ["input/TeX","output/CommonHTML"],
 extensions: ["tex2jax.js"],
 TeX: {
   extensions: ["AMSmath.js","AMSsymbols.js"],
   equationNumbers: {
     autoNumber: "AMS"
   }
 },
 tex2jax: {
   inlineMath: [["\\(", "\\)"]],
   displayMath: [["\\[", "\\]"]]
 }
});</script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
 --light-background: url('/img/bk.jpg');
 --theme-encrypt-confirm: '确认'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
  menuSettings: {
    zoom: "None"
  },
  showMathMenu: false,
  jax: ["input/TeX","output/CommonHTML"],
  extensions: ["tex2jax.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js"],
    equationNumbers: {
      autoNumber: "AMS"
    }
  },
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]]
  }
});
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/proverb/"><span class="navItemTitle">Proverb</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>LLM Deepseek相关</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2025-01-27T03:30:58.805Z" id="date"> 2025-01-27</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2025-02-21T14:17:59.590Z" id="updated"> 2025-02-21</time></div></span></div></div><hr><div id="post-content"><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2>
<ul>
<li>token 一个词语或者词组，--(embedding)
---&gt;对应一个向量，表示含义</li>
<li>每次根据前文预测下一个词语</li>
<li>Attention层+多层感知层+Attention层+.....</li>
<li>Attention层：pass information around the token into its vector
<ul>
<li>查询矩阵 * 嵌入向量 -&gt; 查询向量 （like looking for adj of
it，降维)</li>
<li>键向量，回答查询 （降维）</li>
<li>每个查询向量和键向量的相似度（ <span
class="math inline">\(n^2\)</span>，n为上下文长度 ) -&gt; the embeddings
of x attend to the embedding of y</li>
<li>值矩阵 * 嵌入向量：值向量 （shape与嵌入向量相同）</li>
<li>值向量用相关性为权重进行加权相加
（每个位置只被此位置前面的token影响：掩码）-&gt; <span
class="math inline">\(\Delta E\)</span></li>
<li>值参数矩阵太大 -&gt; 变成两个参数矩阵相乘 （<span
class="math inline">\(V_{up} * V_{down} (* E)\)</span>)</li>
<li>单头和多头 不同的注意力模式（W，K，V参数矩阵不一样）-&gt; 所有 <span
class="math inline">\(\Delta E\)</span> 相加
<ul>
<li>V_up 合起来为输出矩阵</li>
</ul></li>
<li>自注意力和交叉注意力</li>
</ul></li>
<li>多层感知层：每个token单独处理，存储事实
<ul>
<li><span class="math inline">\(W_{up} \times E + B_{up}\)</span>
（注意升维了）<br />
</li>
<li>ReLU函数激活（得到的值叫做神经元）</li>
<li><span class="math inline">\(W_{down} \times E + B_{down}\)</span>
（注意降维了）</li>
<li>事实存储在wup的行向量（像embedding一样代表一种特征）和对应的wdown的列向量的关系中</li>
</ul></li>
</ul>
<p class='item-img' data-src='/asset/logo.png'><img src="/asset/logo.png" /></p>
<p class='item-img' data-src='/asset/Pasted%20image%2020250127153936.png'><img src="/asset/Pasted%20image%2020250127153936.png" /> ##
Deepseek原理 - MLE多头潜在注意力 = 多头自注意力+低秩联合压缩 - MoE
每个token激活8个专家，共享专家+路由专家 -
<strong>辅助损失的主要作用</strong> 是在训练过程中鼓励 token
更均匀地分配给不同的专家。无辅助损失的负载均衡策略：采用
<strong>动态偏置项（Bias Term）</strong>
来调整专家的选择概率。监控每个专家的负载来调整概率。 - MTP
同时预测多个后续token - train： - PP：流水线 -&gt; DualPipe -
EP：专家并行 - DP：数据并行 - 大多数核心计算内核，即 GEMM（General
Matrix Multiply）操作，以 FP8 精度实现。保持以下组件的原始精度（例如
BF16 或 FP32）：嵌入模块、输出头、MoE
门控模块、归一化操作符和注意力操作符。 - Adam
它是一种自动调整学习率的算法，优化器 - <strong>Wgrad
操作</strong>（权重反向传播，Weight Gradient） - 通信带宽是 MoE
模型训练的关键瓶颈。 ### 部署 -
“部署”指的是将训练好的模型应用到实际场景中的过程，使其能够在特定环境中运行并提供服务
-
<strong>MoE的全对全通信</strong>：<strong>跨节点通信</strong>使用高速网络（如InfiniBand，IB）；<strong>节点内通信</strong>在节点内部，通过GPU之间的NVLink
-
预填充和解码：推理的两个阶段，预填充是用prompt生成第一个输出token，解码是一个一个输出token。预填充阶段计算密集，适合高性能计算资源；解码阶段计算稀疏，适合低延迟优化
- KV
Cache：在自回归生成任务中，模型每次生成一个新token时，都需要计算当前token与之前所有token之间的注意力关系。KV
Cache
通过缓存之前计算过的K和V矩阵，避免重复计算，从而显著提高推理效率。</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2025/02/21/%E6%80%BB%E7%BB%93/%E5%A4%A7%E4%BA%8C%E4%B8%8A%E6%80%BB%E7%BB%93/">← 下一篇 大二上总结</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2025/01/20/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/test/">test 上一篇 →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://cdn.luogu.com.cn/upload/image_hosting/8cc2fgam.png" alt="Logo" style="margin:0;border-radius:0;"></a><h1 id="Dr"><a href="/">lyx12138</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">基本原理</span></a></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>